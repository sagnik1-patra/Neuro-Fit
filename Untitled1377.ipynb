{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887aaaff-f619-4003-928c-433b313a2778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --pkl PKL --model MODEL --input INPUT --out OUT [--topk TOPK] [--ids IDS] [--target TARGET]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --pkl, --model, --input, --out\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "# predict.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Silence TF/keras info logs if desired\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Keras safe loader\n",
    "# -----------------------------\n",
    "try:\n",
    "    # TF/Keras >= 3\n",
    "    from keras.src.saving import saving_api as keras_saving_api\n",
    "    from keras.src.models import load_model as keras_load_model  # type: ignore\n",
    "except Exception:\n",
    "    try:\n",
    "        # TF/Keras 2.x\n",
    "        from keras.models import load_model as keras_load_model  # type: ignore\n",
    "    except Exception as e:\n",
    "        keras_load_model = None\n",
    "\n",
    "def load_keras_model_safely(path: str):\n",
    "    \"\"\"\n",
    "    Load a Keras model without trying to re-compile (avoids 'mse' missing, etc.).\n",
    "    Works for .h5 or .keras. Raises if unsupported.\n",
    "    \"\"\"\n",
    "    if keras_load_model is None:\n",
    "        raise RuntimeError(\"Could not import keras load_model.\")\n",
    "    # compile=False prevents deserializing losses/metrics like 'mse' you don't need for inference\n",
    "    return keras_load_model(path, compile=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helpers & transformers\n",
    "# -----------------------------\n",
    "\n",
    "_dt_part = r\"(year|month|day|dow|hour)\"\n",
    "_double_dt_re = re.compile(rf\"^(.*)__{_dt_part}$\")\n",
    "\n",
    "def sanitize_datetime_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize any accidental double-underscore datetime features:\n",
    "    'col__year' -> 'col_year'. If both exist, drop the double-underscore version.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(df) if not isinstance(df, pd.DataFrame) else df\n",
    "    rename_map, drop_cols = {}, []\n",
    "    for c in list(df.columns):\n",
    "        m = _double_dt_re.match(c)\n",
    "        if m:\n",
    "            base, part = m.group(1), m.group(2)\n",
    "            fixed = f\"{base}_{part}\"\n",
    "            if fixed in df.columns:\n",
    "                drop_cols.append(c)\n",
    "            else:\n",
    "                rename_map[c] = fixed\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    return df\n",
    "\n",
    "# Minimal transformer class used by the fitted ColumnTransformer\n",
    "# (must be present to unpickle the preprocess pipeline).\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DateTimeExpand(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Matches the class used during fit: expands datetime cols into\n",
    "    <col>_year/_month/_day/_dow/_hour (single underscore).\n",
    "    \"\"\"\n",
    "    def __init__(self, features: Optional[List[str]] = None):\n",
    "        self.features = features if features is not None else [\"year\", \"month\", \"day\", \"dow\", \"hour\"]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        import pandas as pd\n",
    "        X = pd.DataFrame(X)\n",
    "        self.cols_ = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        import pandas as pd\n",
    "        X = pd.DataFrame(X)\n",
    "        out = {}\n",
    "        for col in self.cols_:\n",
    "            dt = pd.to_datetime(X[col], errors=\"coerce\", utc=False)\n",
    "            if \"year\" in self.features:\n",
    "                out[f\"{col}_year\"] = dt.dt.year.astype(\"Int64\")\n",
    "            if \"month\" in self.features:\n",
    "                out[f\"{col}_month\"] = dt.dt.month.astype(\"Int64\")\n",
    "            if \"day\" in self.features:\n",
    "                out[f\"{col}_day\"] = dt.dt.day.astype(\"Int64\")\n",
    "            if \"dow\" in self.features:\n",
    "                out[f\"{col}_dow\"] = dt.dt.dayofweek.astype(\"Int64\")\n",
    "            if \"hour\" in self.features:\n",
    "                out[f\"{col}_hour\"] = dt.dt.hour.astype(\"Int64\")\n",
    "        Xout = pd.DataFrame(out).fillna(-1)\n",
    "        return Xout.values\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        names = []\n",
    "        for col in getattr(self, \"cols_\", []):\n",
    "            if \"year\" in self.features:  names.append(f\"{col}_year\")\n",
    "            if \"month\" in self.features: names.append(f\"{col}_month\")\n",
    "            if \"day\" in self.features:   names.append(f\"{col}_day\")\n",
    "            if \"dow\" in self.features:   names.append(f\"{col}_dow\")\n",
    "            if \"hour\" in self.features:  names.append(f\"{col}_hour\")\n",
    "        return np.array(names, dtype=object)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) I/O & prediction routines\n",
    "# -----------------------------\n",
    "\n",
    "def _ensure_dense_small(X, max_dense_features: int = 200_000):\n",
    "    \"\"\"\n",
    "    If X is sparse and tiny, convert to dense to avoid Keras sparse issues.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import scipy.sparse as sp\n",
    "        if sp.issparse(X):\n",
    "            nfeat = X.shape[1]\n",
    "            if nfeat <= max_dense_features:\n",
    "                return X.toarray()\n",
    "        return X\n",
    "    except Exception:\n",
    "        return X\n",
    "\n",
    "def _align_to_fit_columns(df: pd.DataFrame, preprocess) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure df has exactly the raw columns (feature_names_in_) the fitted\n",
    "    ColumnTransformer expects: re-order, add missing as NaN, drop extras.\n",
    "    \"\"\"\n",
    "    if not hasattr(preprocess, \"feature_names_in_\"):\n",
    "        # Older scikit-learn may not set this; in that case assume current df columns are fine.\n",
    "        return df\n",
    "\n",
    "    expected = list(preprocess.feature_names_in_)\n",
    "    cur = set(df.columns)\n",
    "    # Add missing columns as NaN\n",
    "    for c in expected:\n",
    "        if c not in cur:\n",
    "            df[c] = np.nan\n",
    "    # Drop unexpected columns\n",
    "    df = df[expected]\n",
    "    return df\n",
    "\n",
    "def _topk_from_probs(probs: np.ndarray, topk: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return (topk_indices, topk_probs) for each row.\n",
    "    \"\"\"\n",
    "    if probs.ndim == 1:\n",
    "        probs = probs[:, None]\n",
    "    k = min(topk, probs.shape[1])\n",
    "    top_idx = np.argpartition(-probs, kth=k-1, axis=1)[:, :k]\n",
    "    # sort those top-k\n",
    "    row_indices = np.arange(probs.shape[0])[:, None]\n",
    "    sorted_order = np.argsort(-probs[row_indices, top_idx], axis=1)\n",
    "    top_sorted_idx = top_idx[row_indices, sorted_order]\n",
    "    top_sorted_probs = probs[row_indices, top_sorted_idx]\n",
    "    return top_sorted_idx, top_sorted_probs\n",
    "\n",
    "def load_bundle(pkl_path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(pkl_path):\n",
    "        raise FileNotFoundError(pkl_path)\n",
    "    # DateTimeExpand is defined above so unpickling works\n",
    "    bundle = joblib.load(pkl_path)\n",
    "    if \"preprocess\" not in bundle:\n",
    "        raise KeyError(\"Bundle missing 'preprocess'\")\n",
    "    return bundle\n",
    "\n",
    "def predict_file(\n",
    "    input_csv: str,\n",
    "    pkl_path: str,\n",
    "    model_path: str,\n",
    "    out_csv: str,\n",
    "    topk: int = 5,\n",
    "    id_cols: Optional[List[str]] = None,\n",
    "    target_col: Optional[str] = None,\n",
    "    print_out: bool = True,\n",
    "):\n",
    "    # Load bundle and model\n",
    "    print(\"[INFO] Loading preprocess bundle...\")\n",
    "    bundle = load_bundle(pkl_path)\n",
    "    preprocess = bundle[\"preprocess\"]\n",
    "    label_encoder = bundle.get(\"label_encoder\", None)\n",
    "    class_names = bundle.get(\"class_names\", None)\n",
    "    target_col_bundle = bundle.get(\"target_col\", None)\n",
    "    if target_col is None and target_col_bundle is not None:\n",
    "        target_col = target_col_bundle\n",
    "\n",
    "    print(\"[INFO] Loading model...\")\n",
    "    model = load_keras_model_safely(model_path)\n",
    "\n",
    "    # Read input\n",
    "    print(f\"[INFO] Reading input: {input_csv}\")\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Drop target if present\n",
    "    if target_col and target_col in df.columns:\n",
    "        df = df.drop(columns=[target_col])\n",
    "\n",
    "    # Normalize accidental datetime double-underscore artifacts\n",
    "    df = sanitize_datetime_columns(df)\n",
    "\n",
    "    # Align raw columns to what the pipeline expects\n",
    "    df = _align_to_fit_columns(df, preprocess)\n",
    "\n",
    "    # Transform\n",
    "    print(\"[INFO] Transforming inputs...\")\n",
    "    X_proc = preprocess.transform(df)\n",
    "    X_proc = _ensure_dense_small(X_proc)\n",
    "\n",
    "    # Predict probabilities or logits\n",
    "    print(\"[INFO] Predicting...\")\n",
    "    raw = model.predict(X_proc, verbose=0)\n",
    "    # Try to softmax if they look like logits\n",
    "    probs = raw\n",
    "    if raw.ndim == 2 and raw.shape[1] > 1:\n",
    "        # normalize row-wise if not already in [0,1]\n",
    "        row_sums = raw.max(axis=1)\n",
    "        if np.any(row_sums > 1.0):\n",
    "            # assume logits\n",
    "            e = np.exp(raw - raw.max(axis=1, keepdims=True))\n",
    "            probs = e / e.sum(axis=1, keepdims=True)\n",
    "    else:\n",
    "        # binary or regression fallback; treat as probability of class 1 if binary\n",
    "        probs = np.hstack([1 - raw, raw]) if raw.ndim == 2 else raw\n",
    "\n",
    "    # Map top-k to labels\n",
    "    top_idx, top_p = _topk_from_probs(probs, topk=topk)\n",
    "    n = probs.shape[0]\n",
    "    out_rows = []\n",
    "\n",
    "    # Build label mapping\n",
    "    def idx_to_label(i: int) -> Union[str, int]:\n",
    "        if label_encoder is not None and hasattr(label_encoder, \"inverse_transform\"):\n",
    "            # inverse_transform expects array-like\n",
    "            return label_encoder.inverse_transform([i])[0]\n",
    "        elif class_names is not None and 0 <= i < len(class_names):\n",
    "            return class_names[i]\n",
    "        else:\n",
    "            return int(i)  # fallback to class index\n",
    "\n",
    "    # Prepare output DataFrame\n",
    "    base_cols = []\n",
    "    if id_cols:\n",
    "        for c in id_cols:\n",
    "            if c in df.columns:\n",
    "                base_cols.append(c)\n",
    "\n",
    "    for r in range(n):\n",
    "        row = {}\n",
    "        for c in base_cols:\n",
    "            row[c] = df.iloc[r][c]\n",
    "        # top-k label/prob pairs\n",
    "        for k in range(top_idx.shape[1]):\n",
    "            cls_i = int(top_idx[r, k])\n",
    "            row[f\"pred_top{k+1}_label\"] = idx_to_label(cls_i)\n",
    "            row[f\"pred_top{k+1}_prob\"] = float(top_p[r, k])\n",
    "        out_rows.append(row)\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    print(f\"[INFO] Wrote predictions to: {out_csv}\")\n",
    "\n",
    "    if print_out:\n",
    "        print(out_df.head(min(10, len(out_df))).to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) CLI\n",
    "# -----------------------------\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser(description=\"Predict with fitted preprocess + Keras model.\")\n",
    "    ap.add_argument(\"--pkl\",   required=True, help=\"Path to preprocess_bundle.pkl\")\n",
    "    ap.add_argument(\"--model\", required=True, help=\"Path to Keras model (.h5 or .keras)\")\n",
    "    ap.add_argument(\"--input\", required=True, help=\"CSV to score\")\n",
    "    ap.add_argument(\"--out\",   required=True, help=\"Output CSV with predictions\")\n",
    "    ap.add_argument(\"--topk\",  type=int, default=5, help=\"Top-K classes to output\")\n",
    "    ap.add_argument(\"--ids\",   default=\"\", help=\"Comma-separated id columns to copy into output (optional)\")\n",
    "    ap.add_argument(\"--target\", default=\"\", help=\"Target column name if present in input (will be dropped)\")\n",
    "    return ap.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    id_cols = [c.strip() for c in args.ids.split(\",\") if c.strip()] if args.ids else None\n",
    "    target_col = args.target.strip() or None\n",
    "    predict_file(\n",
    "        input_csv=args.input,\n",
    "        pkl_path=args.pkl,\n",
    "        model_path=args.model,\n",
    "        out_csv=args.out,\n",
    "        topk=args.topk,\n",
    "        id_cols=id_cols,\n",
    "        target_col=target_col,\n",
    "        print_out=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a4bc8-cd88-4f17-a499-37013fcf8aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
