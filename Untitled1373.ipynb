{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d945c6bd-b0b5-4681-baf3-024e70fd3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Keys in PKL: ['preprocess', 'task_type', 'target_col', 'numeric_cols', 'cat_cols', 'text_cols', 'datetime_cols']\n",
      "[WARN] No label_encoder in PKL – will fit one from dataset (order may differ from training)\n",
      "[INFO] #classes: 9492  | #samples: 80000\n",
      "[INFO] Class distribution (encoded) – showing first 10: {2763: 14, 4646: 8, 2349: 8, 6362: 5, 8183: 4, 2263: 6, 5128: 13, 7931: 14, 4278: 14, 3585: 11}\n",
      "[WARN] Some classes have <2 samples; using non-stratified split.\n",
      "[SAVE] C:\\Users\\sagni\\Downloads\\Neuro Fit\\loss_curve.png\n",
      "[WARN] Model outputs 1 classes, encoder has 9492. Metrics may not align.\n",
      "\n",
      "=== Top-K Accuracy ===\n",
      "Top-1 accuracy: 0.0022\n",
      "Top-5 accuracy: 0.0022\n",
      "[SAVE] C:\\Users\\sagni\\Downloads\\Neuro Fit\\topk_metrics.txt\n",
      "[SAVE] C:\\Users\\sagni\\Downloads\\Neuro Fit\\cm_topN_counts.png\n",
      "[SAVE] C:\\Users\\sagni\\Downloads\\Neuro Fit\\cm_topN_norm.png\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       100.0       0.05      1.00      0.09        35\n",
      "         0.0       0.00      0.00      0.00       547\n",
      "        38.2       0.00      0.00      0.00         7\n",
      "       41.74       0.00      0.00      0.00        10\n",
      "       50.09       0.00      0.00      0.00         7\n",
      "       71.47       0.00      0.00      0.00        10\n",
      "       69.08       0.00      0.00      0.00         7\n",
      "       56.77       0.00      0.00      0.00         8\n",
      "       50.08       0.00      0.00      0.00         9\n",
      "       57.17       0.00      0.00      0.00         7\n",
      "       54.62       0.00      0.00      0.00         7\n",
      "       38.97       0.00      0.00      0.00         7\n",
      "       60.81       0.00      0.00      0.00         7\n",
      "       55.03       0.00      0.00      0.00         8\n",
      "       27.11       0.00      0.00      0.00         8\n",
      "       55.09       0.00      0.00      0.00         7\n",
      "       50.88       0.00      0.00      0.00         9\n",
      "       77.72       0.00      0.00      0.00         7\n",
      "       70.63       0.00      0.00      0.00         9\n",
      "        46.9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.05       723\n",
      "   macro avg       0.00      0.05      0.00       723\n",
      "weighted avg       0.00      0.05      0.00       723\n",
      "\n",
      "[SAVE] C:\\Users\\sagni\\Downloads\\Neuro Fit\\classification_report_topN.txt\n"
     ]
    }
   ],
   "source": [
    "# === evaluate_neurofit_topk_cm_fixed.py ===\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, joblib\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "BASE_DIR  = r\"C:\\Users\\sagni\\Downloads\\Neuro Fit\"\n",
    "CSV_PATH  = os.path.join(BASE_DIR, \"archive\", \"human_cognitive_performance.csv\")\n",
    "H5_PATH   = os.path.join(BASE_DIR, \"neurofit_model.h5\")\n",
    "PKL_PATH  = os.path.join(BASE_DIR, \"neurofit_preprocess.pkl\")\n",
    "HIST_CSV  = os.path.join(BASE_DIR, \"training_history.csv\")  # optional\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column): self.column = column\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X[[self.column]]\n",
    "\n",
    "class To1DString(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.iloc[:, 0].astype(str).values\n",
    "        return np.asarray(X).astype(str).ravel()\n",
    "\n",
    "class DateTimeExpand(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns): self.columns = columns\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        outs = []\n",
    "        for c in self.columns:\n",
    "            s = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "            outs.append(pd.DataFrame({\n",
    "                f\"{c}_year\":  s.dt.year.fillna(0).astype(int),\n",
    "                f\"{c}_month\": s.dt.month.fillna(0).astype(int),\n",
    "                f\"{c}_day\":   s.dt.day.fillna(0).astype(int),\n",
    "                f\"{c}_dow\":   s.dt.dayofweek.fillna(0).astype(int),\n",
    "                f\"{c}_hour\":  s.dt.hour.fillna(0).astype(int),\n",
    "            }))\n",
    "        return pd.concat(outs, axis=1) if outs else np.empty((len(X), 0))\n",
    "\n",
    "def ensure_dense_if_small(X, max_feats=50000):\n",
    "    if hasattr(X, \"toarray\") and X.shape[1] <= max_feats:\n",
    "        return X.toarray()\n",
    "    return X\n",
    "\n",
    "def plot_curve(epochs, y_tr, y_va, title, ylabel, out_path):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs, y_tr, label=\"Train\")\n",
    "    if y_va is not None: plt.plot(epochs, y_va, label=\"Val\")\n",
    "    plt.title(title); plt.xlabel(\"Epoch\"); plt.ylabel(ylabel)\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "    print(f\"[SAVE] {out_path}\")\n",
    "\n",
    "def plot_cm(cm_mat, labels, title, path, fmt=\"d\", cmap=\"Blues\"):\n",
    "    plt.figure(figsize=(8,7))\n",
    "    sns.heatmap(cm_mat, annot=True, fmt=fmt, cmap=cmap,\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    print(f\"[SAVE] {path}\")\n",
    "\n",
    "# --- load artifacts ---\n",
    "if not os.path.exists(PKL_PATH): raise FileNotFoundError(PKL_PATH)\n",
    "if not os.path.exists(H5_PATH):  raise FileNotFoundError(H5_PATH)\n",
    "bundle = joblib.load(PKL_PATH)\n",
    "print(\"[INFO] Keys in PKL:\", list(bundle.keys()))\n",
    "preprocess     = bundle.get(\"preprocess\")\n",
    "target_col     = bundle.get(\"target_col\")\n",
    "datetime_cols  = bundle.get(\"datetime_cols\", [])\n",
    "label_encoder  = bundle.get(\"label_encoder\", None)\n",
    "if label_encoder is None:\n",
    "    print(\"[WARN] No label_encoder in PKL – will fit one from dataset (order may differ from training)\")\n",
    "\n",
    "# --- load data ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "for c in datetime_cols:\n",
    "    if c in df.columns: df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not in CSV. Available: {list(df.columns)}\")\n",
    "\n",
    "X_df = df.drop(columns=[target_col])\n",
    "y_raw = df[target_col].astype(str)\n",
    "if label_encoder is None: label_encoder = LabelEncoder().fit(y_raw)\n",
    "y = label_encoder.transform(y_raw)\n",
    "class_names = [str(c) for c in label_encoder.classes_]\n",
    "n_classes = len(class_names)\n",
    "\n",
    "counts = Counter(y)\n",
    "print(\"[INFO] #classes:\", n_classes, \" | #samples:\", len(y))\n",
    "print(\"[INFO] Class distribution (encoded) – showing first 10:\", dict(list(counts.items())[:10]))\n",
    "use_stratify = min(counts.values()) >= 2 if n_classes > 1 else False\n",
    "if not use_stratify: print(\"[WARN] Some classes have <2 samples; using non-stratified split.\")\n",
    "\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X_df, y, test_size=0.2, random_state=42, stratify=y if use_stratify else None\n",
    ")\n",
    "\n",
    "X_test = preprocess.transform(X_test_df)\n",
    "X_test = ensure_dense_if_small(X_test)\n",
    "\n",
    "# >>> FIX: load without compiling (avoids 'mse' deserialization error)\n",
    "model = load_model(H5_PATH, compile=False)\n",
    "\n",
    "# --- optional curves ---\n",
    "if os.path.exists(HIST_CSV):\n",
    "    hist = pd.read_csv(HIST_CSV)\n",
    "    epochs = hist[\"epoch\"] if \"epoch\" in hist.columns else np.arange(1, len(hist)+1)\n",
    "    acc_col = \"accuracy\" if \"accuracy\" in hist.columns else (\"acc\" if \"acc\" in hist.columns else None)\n",
    "    val_acc_col = \"val_accuracy\" if \"val_accuracy\" in hist.columns else (\"val_acc\" if \"val_acc\" in hist.columns else None)\n",
    "    if acc_col:\n",
    "        plot_curve(epochs, hist[acc_col],\n",
    "                   hist[val_acc_col] if (val_acc_col and val_acc_col in hist.columns) else None,\n",
    "                   \"Model Accuracy\", \"Accuracy\", os.path.join(BASE_DIR, \"accuracy_curve.png\"))\n",
    "    if \"loss\" in hist.columns:\n",
    "        plot_curve(epochs, hist[\"loss\"],\n",
    "                   hist[\"val_loss\"] if \"val_loss\" in hist.columns else None,\n",
    "                   \"Model Loss\", \"Loss\", os.path.join(BASE_DIR, \"loss_curve.png\"))\n",
    "else:\n",
    "    print(\"[INFO] training_history.csv not found – skipping curves\")\n",
    "\n",
    "# --- predictions + top-k ---\n",
    "probs = model.predict(X_test, verbose=0)\n",
    "if probs.ndim == 1:\n",
    "    probs = np.stack([1 - probs, probs], axis=1)\n",
    "\n",
    "num_model_classes = probs.shape[1]\n",
    "if num_model_classes != n_classes:\n",
    "    print(f\"[WARN] Model outputs {num_model_classes} classes, encoder has {n_classes}. Metrics may not align.\")\n",
    "\n",
    "top1_preds = np.argmax(probs, axis=1)\n",
    "top1_acc = (top1_preds == y_test).mean()\n",
    "\n",
    "def topk_acc(probs, y_true, k=5):\n",
    "    k = min(k, probs.shape[1])\n",
    "    topk = np.argpartition(-probs, kth=k-1, axis=1)[:, :k]\n",
    "    return np.mean([y_true[i] in topk[i] for i in range(len(y_true))])\n",
    "\n",
    "top5_acc = topk_acc(probs, y_test, k=5)\n",
    "print(f\"\\n=== Top-K Accuracy ===\\nTop-1 accuracy: {top1_acc:.4f}\\nTop-5 accuracy: {top5_acc:.4f}\")\n",
    "with open(os.path.join(BASE_DIR, \"topk_metrics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Top-1 accuracy: {top1_acc:.6f}\\nTop-5 accuracy: {top5_acc:.6f}\\n\")\n",
    "print(f\"[SAVE] {os.path.join(BASE_DIR, 'topk_metrics.txt')}\")\n",
    "\n",
    "# --- Confusion Matrix on Top-N only ---\n",
    "TOP_N = 20\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_counts = Counter(y_test)\n",
    "topN_ids = [c for c, _ in test_counts.most_common(TOP_N)]\n",
    "other_id = max(n_classes, probs.shape[1]) + 1\n",
    "\n",
    "def map_to_topN(y_arr, top_ids, other_label):\n",
    "    return np.array([yi if yi in top_ids else other_label for yi in y_arr], dtype=int)\n",
    "\n",
    "y_test_top = map_to_topN(y_test, topN_ids, other_id)\n",
    "y_pred_top = map_to_topN(top1_preds, topN_ids, other_id)\n",
    "labels_for_cm_ids = topN_ids + [other_id]\n",
    "labels_for_cm_names = [class_names[i] for i in topN_ids] + [\"Other\"]\n",
    "\n",
    "cm = confusion_matrix(y_test_top, y_pred_top, labels=labels_for_cm_ids)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "plot_cm(cm, labels_for_cm_names, f\"Confusion Matrix (Top {TOP_N} + Other)\",\n",
    "        os.path.join(BASE_DIR, \"cm_topN_counts.png\"), fmt=\"d\", cmap=\"Blues\")\n",
    "plot_cm(cm_norm, labels_for_cm_names, f\"Confusion Matrix (Top {TOP_N} + Other, row-norm)\",\n",
    "        os.path.join(BASE_DIR, \"cm_topN_norm.png\"), fmt=\".2f\", cmap=\"Greens\")\n",
    "\n",
    "# optional: per-class report for Top-N only\n",
    "mask_top = (y_test_top != other_id)\n",
    "if mask_top.any():\n",
    "    rep = classification_report(\n",
    "        y_test_top[mask_top], y_pred_top[mask_top],\n",
    "        target_names=[class_names[i] for i in topN_ids],\n",
    "        zero_division=0\n",
    "    )\n",
    "    with open(os.path.join(BASE_DIR, \"classification_report_topN.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(rep)\n",
    "    print(rep)\n",
    "    print(f\"[SAVE] {os.path.join(BASE_DIR, 'classification_report_topN.txt')}\")\n",
    "else:\n",
    "    print(\"[INFO] No samples from Top-N classes in test split; skipped Top-N report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ca73f-606d-4517-bde3-dc02f6be457e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
